{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text pre-processing in Python",
   "id": "410a2fc746cbfab3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook demonstrates text preprocessing using Python. It covers basic built-in methods, NLTK, and SpaCy for common preprocessing tasks such as:\n",
    "\n",
    "- Converting text to lowercase\n",
    "- Removing punctuation\n",
    "- Tokenization\n",
    "- Lemmatization\n",
    "- Stopword removal"
   ],
   "id": "c08c11e23fbd6e74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install required packages\n",
    "%pip install nltk spacy"
   ],
   "id": "3d32a724763938cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example text\n",
    "text = \"This is an Example TEXT with Mixed CASE and Punctuations!!!\"\n",
    "print(text)"
   ],
   "id": "d4df48c6f36300f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Basic Pre-processing with Python Built-in Methods",
   "id": "a5977827724bbbbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lowercase_text = text.lower()\n",
    "print(\"Lowercase:\", lowercase_text)\n",
    "\n",
    "# Remove punctuations\n",
    "import string\n",
    "no_punctuation = ''.join(char for char in text if char not in string.punctuation)\n",
    "print(\"Without Punctuation:\", no_punctuation)\n",
    "\n",
    "# Split into words\n",
    "words = no_punctuation.split()\n",
    "print(\"Words:\", words)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Basic Preprocessing with NLTK",
   "id": "95aa72ef4ceb748d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import NLTK and download required resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Without Stopwords:\", filtered_tokens)"
   ],
   "id": "561cdc756f4eaf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Basic Preprocessing with Spacy",
   "id": "e6be4cb3d836ce6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load spaCy\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")  # Download the English model if not already installed\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "print(\"Lemmatized:\", lemmatized)\n",
    "\n",
    "# Remove stopwords and punctuations\n",
    "filtered = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "print(\"Filtered:\", filtered)"
   ],
   "id": "df0c5d8515910948",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
