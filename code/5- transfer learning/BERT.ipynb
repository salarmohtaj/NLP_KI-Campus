{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transfer Learning with BERT for Text Classification\n",
    "\n",
    "This notebook demonstrates how to leverage the pre-trained BERT model for a text classification task using transfer learning. \n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art model designed for natural language understanding.\n",
    "We fine-tune BERT on a simple binary sentiment classification dataset, train it for three epochs, and evaluate its performance."
   ],
   "id": "d8241ac5b6e3d4f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "69a9f9d4d7f406ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for GPU availability\n",
    "# Use GPU if available; otherwise, fallback to CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "6f224f6c8e494e41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a simple dataset (can be replaced with any other text classification dataset)\n",
    "data = {\n",
    "    'text': [\n",
    "        \"I love this product! It's amazing.\",  # Positive sentiment\n",
    "        \"Terrible experience, would not recommend.\",  # Negative sentiment\n",
    "        \"Great value for the money.\",  # Positive sentiment\n",
    "        \"The worst item I have ever purchased.\",  # Negative sentiment\n",
    "        \"Good quality and fast shipping.\",  # Positive sentiment\n",
    "        \"Awful customer service, very disappointed.\"  # Negative sentiment\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "# 80% of the data is used for training, and 20% is used for validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42\n",
    ")"
   ],
   "id": "1eb4a8bc5ca983bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load BERT tokenizer\n",
    "# The tokenizer is used to preprocess the text data for the BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a custom dataset class to handle tokenization and data loading\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts  # Input texts\n",
    "        self.labels = labels  # Corresponding labels\n",
    "        # Tokenize the texts\n",
    "        self.encodings = tokenizer(\n",
    "            texts.tolist(),\n",
    "            truncation=True,  # Truncate texts longer than max_length\n",
    "            padding=True,  # Pad shorter texts to max_length\n",
    "            max_length=128,  # Maximum token length\n",
    "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve a single sample\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])  # Add label tensor\n",
    "        return item\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = TextDataset(train_texts, train_labels.tolist())\n",
    "val_dataset = TextDataset(val_texts, val_labels.tolist())\n",
    "\n",
    "# Load data loaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Shuffle training data\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)  # No shuffle for validation data"
   ],
   "id": "bb11af2509423439",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the pre-trained BERT model for sequence classification\n",
    "# num_labels=2 indicates a binary classification task\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)  # Move the model to the selected device\n",
    "\n",
    "# Define the optimizer\n",
    "# AdamW is a commonly used optimizer for transformer models\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, no_deprecation_warning=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3  # Number of epochs\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()  # Reset gradients from the previous step\n",
    "        input_ids = batch['input_ids'].to(device)  # Input token IDs\n",
    "        attention_mask = batch['attention_mask'].to(device)  # Attention mask\n",
    "        labels = batch['labels'].to(device)  # True labels\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model weights\n",
    "\n",
    "        print(f\"Loss: {loss.item():.4f}\", end=\"\\r\")  # Print loss for monitoring\n"
   ],
   "id": "a71090d14048b7f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "facf9deaaf40bb38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)  # Input token IDs\n",
    "        attention_mask = batch['attention_mask'].to(device)  # Attention mask\n",
    "        labels = batch['labels'].to(device)  # True labels\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # Raw predictions\n",
    "        preds = torch.argmax(logits, dim=1)  # Get the predicted class\n",
    "\n",
    "        # Collect predictions and true labels\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Generate and print the classification report\n",
    "# This includes precision, recall, F1-score, and accuracy\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=['Negative', 'Positive']))\n"
   ],
   "id": "899cb93da178f0e4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "NLP"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
