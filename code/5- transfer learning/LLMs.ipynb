{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using LLMs for text classification",
   "id": "d571effa88826952"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the model and tokenizer\n",
    "model_name = 'bigscience/bloom-1b1'\n",
    "\n",
    "# You can also try bigger models like \"EleutherAI/gpt-neo-1.3B\" and \"facebook/opt-1.3b\"\n",
    "# Or replace 'bigscience/bloom-1b1' with smaller models like 'bigscience/bloom-560m' if needed\n",
    "\n",
    " \n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "id": "7bd22f446b00c842"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define a prompt for the language model\n",
    "prompt = (\n",
    "    \"You are an expert sentiment analysis assistant. I will provide you with 5 sentences, and you will evaluate the sentiment of each sentence. \"\n",
    "    \"For each sentence, give a sentiment score between -1 (very negative) and +1 (very positive). \"\n",
    "    \"Here are two examples:\\n\"\n",
    "    \"1. 'I absolutely love this product!' -> Sentiment score: +1\\n\"\n",
    "    \"2. 'This is the worst experience I have ever had.' -> Sentiment score: -1\\n\"\n",
    "    \"Now, evaluate the following sentences:\\n\"\n",
    "    \"1. 'The movie was okay, but it could have been better.'\\n\"\n",
    "    \"2. 'I am thrilled with the service I received today!'\\n\"\n",
    "    \"3. 'The food was cold and tasteless.'\\n\"\n",
    "    \"4. 'What a fantastic day!'\\n\"\n",
    "    \"5. 'I am disappointed with the product quality.'\"\n",
    ")"
   ],
   "id": "5b267aca0e0b5c11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response using the model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=400,  # Limit the number of tokens in the response\n",
    "        temperature=0.7,  # Adjust randomness in the output (0.7 is moderately creative)\n",
    "        do_sample=True  # Enable sampling for more diverse responses\n",
    "    )\n",
    "\n",
    "\n"
   ],
   "id": "8f003e0d32420dee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Decode and print the generated response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Response from LLM:\")\n",
    "print(response)"
   ],
   "id": "3aa969a6eeaff80d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
